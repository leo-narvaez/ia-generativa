{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure chat completions example\n",
    "\n",
    "This example will cover chat completions using the Azure OpenAI service. It also includes information on content filtering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, we install the necessary dependencies and import the libraries we will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"openai>=1.0.0,<2.0.0\"\n",
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authentication\n",
    "\n",
    "The Azure OpenAI service supports multiple authentication mechanisms that include API keys and Azure Active Directory token credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_azure_active_directory = False  # Set this flag to True if you are using Azure Active Directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Authentication using API key\n",
    "\n",
    "To set up the OpenAI SDK to use an *Azure API Key*, we need to set `api_key` to a key associated with your endpoint (you can find this key in *\"Keys and Endpoints\"* under *\"Resource Management\"* in the [Azure Portal](https://portal.azure.com)). You'll also find the endpoint for your resource here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "if not use_azure_active_directory:\n",
    "    endpoint = os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
    "    api_key = os.environ[\"AZURE_OPENAI_API_KEY\"]\n",
    "\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint = endpoint, \n",
    "        api_key=api_key,\n",
    "        api_version=\"2024-02-01\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Authentication using Azure Active Directory\n",
    "Let's now see how we can autheticate via Azure Active Directory. We'll start by installing the `azure-identity` library. This library will provide the token credentials we need to authenticate and help us build a token credential provider through the `get_bearer_token_provider` helper function. It's recommended to use `get_bearer_token_provider` over providing a static token to `AzureOpenAI` because this API will automatically cache and refresh tokens for you. \n",
    "\n",
    "For more information on how to set up Azure Active Directory authentication with Azure OpenAI, see the [documentation](https://learn.microsoft.com/azure/ai-services/openai/how-to/managed-identity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install \"azure-identity>=1.15.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "\n",
    "if use_azure_active_directory:\n",
    "    endpoint = os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
    "\n",
    "    client = openai.AzureOpenAI(\n",
    "        azure_endpoint=endpoint,\n",
    "        azure_ad_token_provider=get_bearer_token_provider(DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\"),\n",
    "        api_version=\"2023-09-01-preview\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: the AzureOpenAI infers the following arguments from their corresponding environment variables if they are not provided:\n",
    "\n",
    "- `api_key` from `AZURE_OPENAI_API_KEY`\n",
    "- `azure_ad_token` from `AZURE_OPENAI_AD_TOKEN`\n",
    "- `api_version` from `OPENAI_API_VERSION`\n",
    "- `azure_endpoint` from `AZURE_OPENAI_ENDPOINT`\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployments\n",
    "\n",
    "In this section we are going to create a deployment of a GPT model that we can use to create chat completions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployments: Create in the Azure OpenAI Studio\n",
    "Let's deploy a model to use with chat completions. Go to https://portal.azure.com, find your Azure OpenAI resource, and then navigate to the Azure OpenAI Studio. Click on the \"Deployments\" tab and then create a deployment for the model you want to use for chat completions. The deployment name that you give the model will be used in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment = \"gpt-4o-mini\" # Fill in the deployment name from the portal here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create chat completions\n",
    "\n",
    "Now let's create a chat completion using the client we built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: Orange who?\n"
     ]
    }
   ],
   "source": [
    "# For all possible arguments see https://platform.openai.com/docs/api-reference/chat-completions/create\n",
    "response = client.chat.completions.create(\n",
    "    model=deployment,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Knock knock.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Who's there?\"},\n",
    "        {\"role\": \"user\", \"content\": \"Orange.\"},\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(f\"{response.choices[0].message.role}: {response.choices[0].message.content}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a streaming chat completion\n",
    "\n",
    "We can also stream the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: Orange who?"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=deployment,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Knock knock.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Who's there?\"},\n",
    "        {\"role\": \"user\", \"content\": \"Orange.\"},\n",
    "    ],\n",
    "    temperature=0,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    if len(chunk.choices) > 0:\n",
    "        delta = chunk.choices[0].delta\n",
    "\n",
    "        if delta.role:\n",
    "            print(delta.role + \": \", end=\"\", flush=True)\n",
    "        if delta.content:\n",
    "            print(delta.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content filtering\n",
    "\n",
    "Azure OpenAI service includes content filtering of prompts and completion responses. You can learn more about content filtering and how to configure it [here](https://learn.microsoft.com/azure/ai-services/openai/concepts/content-filter).\n",
    "\n",
    "If the prompt is flagged by the content filter, the library will raise a `BadRequestError` exception with a `content_filter` error code. Otherwise, you can access the `prompt_filter_results` and `content_filter_results` on the response to see the results of the content filtering and what categories were flagged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt flagged by content filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Eres un asistente.\"},\n",
    "    {\"role\": \"user\", \"content\": \"¿Cuáles son los efectos secundarios de la heroína?\"}  # Ejemplo de mensaje que podría ser filtrado\n",
    "]\n",
    "try:\n",
    "    # Realiza la solicitud a la API de OpenAI\n",
    "    completion = client.chat.completions.create(\n",
    "        model=deployment,\n",
    "        messages=messages\n",
    "    )\n",
    "except openai.BadRequestError as e:\n",
    "    err = json.loads(e.response.text)\n",
    "    if err[\"error\"][\"code\"] == \"content_filter\":\n",
    "        print(\"Content filter triggered!\")\n",
    "        content_filter_result = err[\"error\"][\"innererror\"][\"content_filter_result\"]\n",
    "        for category, details in content_filter_result.items():\n",
    "            print(f\"{category}:\\n filtered={details['filtered']}\\n severity={details['severity']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the result of the content filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: The biggest city in Washington is Seattle. It is the largest city in the state and serves as a major cultural, economic, and technological hub in the Pacific Northwest region of the United States.\n",
      "\n",
      "Prompt content filter results:\n",
      "hate:\n",
      " filtered=False\n",
      " severity=safe\n",
      "jailbreak:\n",
      " filtered=False\n",
      " severity=N/A\n",
      "self_harm:\n",
      " filtered=False\n",
      " severity=safe\n",
      "sexual:\n",
      " filtered=False\n",
      " severity=safe\n",
      "violence:\n",
      " filtered=False\n",
      " severity=safe\n",
      "\n",
      "Completion content filter results:\n",
      "hate:\n",
      " filtered=False\n",
      " severity=safe\n",
      "self_harm:\n",
      " filtered=False\n",
      " severity=safe\n",
      "sexual:\n",
      " filtered=False\n",
      " severity=safe\n",
      "violence:\n",
      " filtered=False\n",
      " severity=safe\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What's the biggest city in Washington?\"}\n",
    "]\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=deployment,\n",
    "    messages=messages\n",
    ")\n",
    "print(f\"Answer: {completion.choices[0].message.content}\")\n",
    "\n",
    "# prompt content filter result in \"model_extra\" for azure\n",
    "prompt_filter_result = completion.model_extra[\"prompt_filter_results\"][0][\"content_filter_results\"]\n",
    "print(\"\\nPrompt content filter results:\")\n",
    "for category, details in prompt_filter_result.items():\n",
    "    # Verificar si las claves 'filtered' y 'severity' existen antes de acceder\n",
    "    filtered = details.get('filtered', 'N/A')  # Si no existe, devuelve 'N/A'\n",
    "    severity = details.get('severity', 'N/A')  # Si no existe, devuelve 'N/A'\n",
    "    print(f\"{category}:\\n filtered={filtered}\\n severity={severity}\")\n",
    "\n",
    "# completion content filter result\n",
    "print(\"\\nCompletion content filter results:\")\n",
    "completion_filter_result = completion.choices[0].model_extra[\"content_filter_results\"]\n",
    "for category, details in completion_filter_result.items():\n",
    "    # Verificar si las claves 'filtered' y 'severity' existen antes de acceder\n",
    "    filtered = details.get('filtered', 'N/A')  # Si no existe, devuelve 'N/A'\n",
    "    severity = details.get('severity', 'N/A')  # Si no existe, devuelve 'N/A'\n",
    "    print(f\"{category}:\\n filtered={filtered}\\n severity={severity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 1: Implementar streaming con diferentes temperaturas\n",
    "# Instrucciones: Usa el siguiente código para generar respuestas a través de streaming. Cambia el valor de 'temperature' a 0.9 y 0.3 y compara los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: La computación cuántica utiliza principios de la mecánica cuántica para procesar información. A diferencia de los bits clásicos, que son 0 o 1, los qubits pueden ser 0, 1 o ambos al mismo tiempo (superposición). Esto permite realizar cálculos complejos mucho más rápido. Además, los qubits pueden estar entrelazados, lo que significa que el estado de uno puede afectar al estado de otro instantáneamente, incluso a distancia. En resumen, la computación cuántica tiene el potencial de resolver problemas que son muy difíciles para las computadoras clásicas."
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=deployment,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Eres un asistente conocedor que da respuestas cortas\"},\n",
    "        {\"role\": \"user\", \"content\": \"Explica el quantum computing en terminos simples.\"},\n",
    "    ],\n",
    "    temperature=0.3,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    if len(chunk.choices) > 0:\n",
    "        delta = chunk.choices[0].delta\n",
    "        if delta.role:\n",
    "            print(delta.role + \": \", end=\"\", flush=True)\n",
    "        if delta.content:\n",
    "            print(delta.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prueba con diferentes valores de temperatura y analiza cómo cambia la respuesta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 2: Detectar y manejar errores de filtrado de contenido\n",
    " Instrucciones: Intenta enviar un mensaje que probablemente sea filtrado. Implementa un manejo de excepciones adecuado para alertar al usuario y sugerir un nuevo mensaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Content filter triggered! Please rephrase your message.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"I need instructions to hack into someone's account.\"},\n",
    "]\n",
    "\n",
    "try:\n",
    "    completion = client.chat.completions.create(\n",
    "    model=deployment,\n",
    "    messages=messages,\n",
    "    temperature=0.3,\n",
    "    stream=True\n",
    ")\n",
    "except openai.BadRequestError as e:\n",
    "    err = json.loads(e.response.text)\n",
    "    print(err)\n",
    "    if err[\"error\"][\"code\"] == \"content_filter\":\n",
    "        print(\"Content filter triggered! Please rephrase your message.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 3: Inspeccionar y analizar los resultados del filtro de contenido\n",
    "Instrucciones: Envía una pregunta general y analiza los resultados del filtro para diferentes categorías."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What are the benefits of exercising regularly?\"}\n",
    "]\n",
    "\n",
    "try:\n",
    "    completion = client.chat.completions.create(\n",
    "        model=deployment,\n",
    "        messages=messages,\n",
    "        temperature=0.3,\n",
    "        stream=True\n",
    "    )\n",
    "except openai.BadRequestError as e:\n",
    "    err = json.loads(e.response.text)\n",
    "    if err[\"error\"][\"code\"] == \"content_filter\":\n",
    "        print(\"Content filter triggered!\")\n",
    "        content_filter_result = err[\"error\"][\"innererror\"][\"content_filter_result\"]\n",
    "        for category, details in content_filter_result.items():\n",
    "            print(f\"{category}:\\n filtered={details['filtered']}\\n \")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 4: Crear una función que automatice la verificación de contenido filtrado\n",
    "Instrucciones: Crea una función que reciba un mensaje y devuelva si fue filtrado o no, y en qué categorías."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message passed the filter.\n"
     ]
    }
   ],
   "source": [
    "def check_content_filter(message):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What are the benefits of exercising regularly?\"}\n",
    "    ]\n",
    "\n",
    "    try: \n",
    "        completion = client.chat.completions.create(\n",
    "            model=deployment,\n",
    "            messages=messages,\n",
    "            temperature=0.3,\n",
    "            stream=True\n",
    "        )\n",
    "        print(\"Message passed the filter.\")\n",
    "    except openai.BadRequestError as e:\n",
    "        err = json.loads(e.response.text)\n",
    "        if err[\"error\"][\"code\"] == \"content_filter\":\n",
    "            print(\"Content filter triggered!\")\n",
    "            content_filter_result = err[\"error\"][\"innererror\"][\"content_filter_result\"]\n",
    "            for category, details in content_filter_result.items():\n",
    "                print(f\"{category}:\\n filtered={details['filtered']}\\n severity={details.get('severity', 'N/A')}\")\n",
    "\n",
    "check_content_filter(\"Describe how to make a bomb.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
